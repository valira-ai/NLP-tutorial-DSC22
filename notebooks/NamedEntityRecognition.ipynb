{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AryboA0vobCI"
      },
      "source": [
        "# Named-Entity Recognition (NER) - Fine-tunning BERT\n",
        "In this notebook we'll take a look at the process of fine-tuning [DistilBERT](https://huggingface.co/distilbert-base-multilingual-cased) model to recognize people, organisations, and locations in text. This will be done using the [conll2003](https://huggingface.co/datasets/conll2003) dataset. The tecniques discussed apply to general NER applications."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first, let's make sure we have a GPU instance in this Colab session:\n",
        "*   `Edit -> Notebook settings -> Hardware accelerator` must be set to **GPU**.\n",
        "*   if needed, reinitiliaze the session by clicking **Connect** in top right corner.\n",
        "\n",
        "After the session is initilized, we can check our assigned GPU with the following command:"
      ],
      "metadata": {
        "id": "noVtSD_AviQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "bACTuHcpvX_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nzZDIWEobCM"
      },
      "source": [
        "Install the Transformers, Datasets, Evaluate and seqeval libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsd12jUtobCN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece] seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "p566asndtC2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "drLqjRO1onfY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "HjSBGaasoqdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "47-08moXo3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][50][\"tokens\"]"
      ],
      "metadata": {
        "id": "Qut-_mDqpUdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"].features[\"pos_tags\"]"
      ],
      "metadata": {
        "id": "tHVyPrZKqauT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names"
      ],
      "metadata": {
        "id": "cABJ1FTay6a-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing (Tokenization and alignment)"
      ],
      "metadata": {
        "id": "q6D26xPgD9fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-multilingual-cased\""
      ],
      "metadata": {
        "id": "RrBoQyKErEKo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "a63Mlc-ArDLY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(raw_datasets[\"train\"][50][\"tokens\"], is_split_into_words=True)\n",
        "inputs"
      ],
      "metadata": {
        "id": "F3KieJN0tvk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "Z9N8fJonuFy5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "IeEWpqc1uaib"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "Jcak0fEFulPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "nnY0oxrnuq4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[\"train\"][0][\"labels\"]"
      ],
      "metadata": {
        "id": "umAeIKivvKEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tunning (Training)"
      ],
      "metadata": {
        "id": "epKFQdVpvoZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "oaeVKBD9vnsD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "nLgjj7e9tjpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ],
      "metadata": {
        "id": "yGGaaDBIvl47"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "qAhNl6L2y3_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    \"distilbert-multilingual-cased-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "b4nCidpyz4AG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "eKOM8PxS0Urj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    \n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "dqS3bQ2H02c-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "X1EB-ApEwBCp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "yBHmcKvg0T5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "rpNk4vrJsqyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])"
      ],
      "metadata": {
        "id": "NjRWa9ZyKRJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_metrics([predictions.predictions, predictions.label_ids])"
      ],
      "metadata": {
        "id": "tlmHXYzNKX-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "tqfYR59GpjgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filler code\n",
        "import torch\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def align_predictions_and_labels(predictions, labels, id2label):\n",
        "  alligned = []\n",
        "  prev_lab = -1\n",
        "  for i, label in enumerate(labels):\n",
        "    if label == None:\n",
        "      continue\n",
        "    if label == prev_lab:\n",
        "      continue\n",
        "    else:\n",
        "      alligned.append(id2label[predictions[i]])\n",
        "      prev_lab = label\n",
        "\n",
        "  return alligned\n",
        "\n",
        "def show_me_some_tags(text, show_legend=True):\n",
        "  model.eval()\n",
        "  words = text.split()\n",
        "  tokenized_inputs = tokenizer(words, truncation=True, is_split_into_words=True)\n",
        "  input_ids = torch.tensor([tokenized_inputs[\"input_ids\"]]).to(device)\n",
        "  attention_mask = torch.tensor([tokenized_inputs[\"attention_mask\"]]).to(device)\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_ids, attention_mask).logits\n",
        "\n",
        "  predictions = np.argmax(logits.cpu().numpy(), axis=2)\n",
        "  predictions = align_predictions_and_labels(predictions[0], tokenized_inputs.word_ids(), id2label)\n",
        "\n",
        "  marked_words = []\n",
        "  for i in range(len(words)):\n",
        "    if predictions[i] == \"O\":\n",
        "      marked_words.append(words[i]) \n",
        "    else:\n",
        "      marked_words.append(\"_\".join([words[i], predictions[i].split(\"-\")[1]])) \n",
        "  print(\" \".join(marked_words))"
      ],
      "metadata": {
        "id": "oUyeHjo8Qk0C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_me_some_tags(\"My name is Luka. I live in Ljubljana.\")"
      ],
      "metadata": {
        "id": "3Mk06uxfPXKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_me_some_tags(\"And this is Andrej. He likes to visit the Faculty of Computer and Infromation Science in Ljubljana\")"
      ],
      "metadata": {
        "id": "-eGVcQatoeVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_me_some_tags(\"Košarkarji Dallasa v letošnji sezoni veliko bolje igrajo proti močnim tekmecem. LA Clippersi so stari znanci in motiva zagovoto ne bo manjkalo.\")"
      ],
      "metadata": {
        "id": "kkC5pi6Apwqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_me_some_tags(\"Luka Dončić je najboljši strelec lige s povprečjem 34,3 točke na tekmo. Ko ima Ljubljančan pravi strelski večer, je Maverickse zelo težko premagati.\")"
      ],
      "metadata": {
        "id": "lSIP01MhqX13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
